{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a1f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# NFL BIG DATA BOWL 2026 - FULLY WORKING COMPREHENSIVE SOLUTION\n",
    "# Complete Implementation with All Features & Visualizations\n",
    "# ================================================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Models\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Preprocessing & Analysis\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\" \"*10 + \"NFL BIG DATA BOWL 2026 - FULLY WORKING SOLUTION\")\n",
    "print(\" \"*15 + \"Complete Implementation with All Features\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# ================================================================================\n",
    "# PART 1: DATA LOADING\n",
    "# ================================================================================\n",
    "print(\"\\nðŸ“Š PART 1: DATA LOADING\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "DATA_DIR = \"/Users/ryanparks/Downloads/compsci_ai/big_data_bowl/nfl-big-data-bowl-2026-prediction/\"\n",
    "\n",
    "# Load data\n",
    "print(\"Loading training data...\")\n",
    "input_files = sorted(glob.glob(os.path.join(DATA_DIR, \"train/input_2023_w*.csv\")))\n",
    "output_files = sorted(glob.glob(os.path.join(DATA_DIR, \"train/output_2023_w*.csv\")))\n",
    "\n",
    "df_in = pd.concat((pd.read_csv(p) for p in tqdm(input_files, desc=\"Input files\")), ignore_index=True)\n",
    "df_out = pd.concat((pd.read_csv(p) for p in tqdm(output_files, desc=\"Output files\")), ignore_index=True)\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "test_in = pd.read_csv(os.path.join(DATA_DIR, \"test_input.csv\"))\n",
    "test_template = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n",
    "supp_path = DATA_DIR + \"supplementary_data.csv\"\n",
    "supplementary = pd.read_csv(supp_path)\n",
    "\n",
    "# Heuristic: contains 'zone' (case-insensitive) -> 'zone'; else 'man'; NaN -> 'unknown'\n",
    "cov_raw = supplementary[\"team_coverage_type\"]\n",
    "cov_str = cov_raw.astype(\"string\").str.lower()\n",
    "coverage_man_zone = pd.Series(\"unknown\", index=supplementary.index, dtype=\"string\")\n",
    "coverage_man_zone = coverage_man_zone.mask(cov_str.notna() & cov_str.str.contains(\"zone\", na=False), \"zone\")\n",
    "coverage_man_zone = coverage_man_zone.mask(cov_str.notna() & ~cov_str.str.contains(\"zone\", na=False), \"man\")\n",
    "\n",
    "supplementary[\"coverage_man_zone\"] = coverage_man_zone\n",
    "supplementary[\"coverage_is_man\"] = (supplementary[\"coverage_man_zone\"] == \"man\").astype(\"int8\")\n",
    "supplementary[\"coverage_is_zone\"] = (supplementary[\"coverage_man_zone\"] == \"zone\").astype(\"int8\")\n",
    "supplementary[\"coverage_is_unknown\"] = (supplementary[\"coverage_man_zone\"] == \"unknown\").astype(\"int8\")\n",
    "\n",
    "# Keep one row per (game_id, play_id) for a clean many:one merge\n",
    "key_cols = [\"game_id\", \"play_id\"]\n",
    "for k in key_cols:\n",
    "    if k not in supplementary.columns:\n",
    "        raise KeyError(f\"'{k}' column is required in supplementary_data.csv for merging.\")\n",
    "supp_play = (supplementary\n",
    "                .drop_duplicates(subset=key_cols, keep=\"first\")\n",
    "                [key_cols + [\"coverage_man_zone\", \"coverage_is_man\", \"coverage_is_zone\", \"coverage_is_unknown\"]]\n",
    "                .reset_index(drop=True))\n",
    "\n",
    "# Merge into train_input / train_output (many frames to one play row)\n",
    "df_in = df_in.merge(\n",
    "    supp_play, on=key_cols, how=\"left\", validate=\"m:1\"\n",
    ")\n",
    "df_out = df_out.merge(\n",
    "    supp_play, on=key_cols, how=\"left\", validate=\"m:1\"\n",
    ")\n",
    "\n",
    "test_in = test_in.merge(\n",
    "    supp_play, on=key_cols, how=\"left\", validate=\"m:1\"\n",
    ")\n",
    "test_template = test_template.merge(\n",
    "    supp_play, on=key_cols, how=\"left\", validate=\"m:1\"\n",
    ")\n",
    "dist = df_in[\"coverage_man_zone\"].value_counts(dropna=False)\n",
    "print(\"[coverage] distribution in train_input:\\n\", dist.to_string())\n",
    "print(f\"\\nâœ“ Data Shapes:\")\n",
    "print(f\"  â€¢ Training inputs: {df_in.shape}\")\n",
    "print(f\"  â€¢ Training outputs: {df_out.shape}\")\n",
    "print(f\"  â€¢ Test inputs: {test_in.shape}\")\n",
    "print(f\"  â€¢ Test template: {test_template.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = df_in.query('play_id == 55 and game_id == 2023112300')\n",
    "player_in = data2.query('nfl_id == 55910')\n",
    "player_out = df_out.query('play_id == 55 and game_id == 2023112300 and nfl_id == 55910')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f024567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_conds(data, goal_list, nfl_id):\n",
    "    df_hist = data.query('nfl_id ==  ' + str(nfl_id))\n",
    "    df_hist = df_hist[[\"x\", \"y\", \"s\", \"a\", \"dir\", \"o\"]]\n",
    "    df_hist['dir_cos'] = np.cos(np.radians(df_hist['dir']))\n",
    "    df_hist['dir_sin'] = np.sin(np.radians(df_hist['dir']))\n",
    "    df_hist['sin_cos'] = df_hist['dir_cos'] **2 + df_hist['dir_sin'] **2\n",
    "    df_hist['sx'] = df_hist['s'] * np.cos(np.radians(df_hist['dir']))\n",
    "    df_hist['sy'] = df_hist['s'] * np.sin(np.radians(df_hist['dir']))\n",
    "    df_hist['s_check'] = np.sqrt(df_hist['sx'] **2 + df_hist['sy'] **2)\n",
    "    goal = goal_list.iloc[-1]\n",
    "    goal_2 = goal_list.iloc[-2]\n",
    "    vx = (goal[0] - goal_2[0]) / 0.1\n",
    "    vy = (goal[1] - goal_2[1]) / 0.1\n",
    "    initial = df_hist.iloc[-1]\n",
    "    conditions = {\n",
    "        'xi': [initial[\"x\"], initial[\"y\"]],\n",
    "        'vi': [initial[\"sx\"], initial[\"sy\"]],\n",
    "        'xf': [goal[0], goal[1]],\n",
    "        'vf': [vx, vy]\n",
    "    }\n",
    "    return df_hist, conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e600bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist = player_in\n",
    "goal_list = player_out[[\"x\", \"y\"]]\n",
    "df_hist = df_hist[[\"x\", \"y\", \"s\", \"a\", \"dir\", \"o\"]]\n",
    "df_hist['dir_cos'] = np.cos(np.radians(df_hist['dir']))\n",
    "df_hist['dir_sin'] = np.sin(np.radians(df_hist['dir']))\n",
    "df_hist['sin_cos'] = df_hist['dir_cos'] **2 + df_hist['dir_sin'] **2\n",
    "df_hist['sx'] = df_hist['s'] * np.cos(np.radians(df_hist['dir']))\n",
    "df_hist['sy'] = df_hist['s'] * np.sin(np.radians(df_hist['dir']))\n",
    "df_hist['s_check'] = np.sqrt(df_hist['sx'] **2 + df_hist['sy'] **2)\n",
    "goal = goal_list.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04448a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matlab.engine\n",
    "eng = matlab.engine.start_matlab()\n",
    "eng.addpath(\"/Users/ryanparks/Downloads/matlab_testing/NLALIB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aabf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng.addpath(\"/Users/ryanparks/Downloads/matlab_testing/NLALIB/OptimTraj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d82b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matlab_player_path(xi, vi2, xf, vf, r):\n",
    "    xi_m = matlab.double([xi], size=(1, 2))\n",
    "    vi_m = matlab.double([vi2], size=(1, 2))\n",
    "    xf_m = matlab.double([xf], size=(1, 2))\n",
    "    vf_m = matlab.double([vf], size=(1, 2))\n",
    "\n",
    "    # (if your MATLAB code expects column vectors, use size=(2,1) instead)\n",
    "\n",
    "    # call the function; set nargout to however many outputs it returns\n",
    "    #out = eng.fastest_path_segment_3(xi_m, vi_m, xf_m, vf_m, float(r), float(401), nargout=1)\n",
    "    out = eng.solveOptimalRunner(xi_m, vi_m, xf_m, vf_m,)\n",
    "    #out = fastest_path_segment_2(xi, vi_2, xm, vm, r, 401);\n",
    "    #out = eng.fastest_path_segment_2(xm, vm, xf, vf, r, 401)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310f6efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matlab_player_path_2(xi, vi2, tfin, m, h, E, F, target):\n",
    "    xi_m = matlab.double([xi], size=(1, 2))\n",
    "    vi_m = matlab.double([vi2], size=(1, 2))\n",
    "    t_m = matlab.double([tfin], size=(1, 1))\n",
    "    m_m = matlab.double([m], size=(1, 1))\n",
    "    h_m = matlab.double([h], size=(1, 1))\n",
    "    E_m = matlab.double([E], size=(1, 1))\n",
    "    F_m = matlab.double([F], size=(1, 1))\n",
    "\n",
    "    target_m = matlab.double([target], size=(1, 2))\n",
    "\n",
    "    # (if your MATLAB code expects column vectors, use size=(2,1) instead)\n",
    "\n",
    "    # call the function; set nargout to however many outputs it returns\n",
    "    #out = eng.fastest_path_segment_3(xi_m, vi_m, xf_m, vf_m, float(r), float(401), nargout=1)\n",
    "    out = eng.energyConstrainedPath(h_m, m_m, E_m, F_m, t_m, xi_m, vi_m, target_m)\n",
    "    #out = fastest_path_segment_2(xi, vi_2, xm, vm, r, 401);\n",
    "    #out = eng.fastest_path_ssolve_segmentegment_2(xm, vm, xf, vf, r, 401)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2394a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_path(X, Y, tf, dt, x_true, y_true):\n",
    "    # Convert to arrays\n",
    "    X = np.array(X).flatten()\n",
    "    Y = np.array(Y).flatten()\n",
    "    n = len(X)\n",
    "    x_true = np.asarray(x_true).ravel()\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "\n",
    "    #t_new = np.arange(0, tf + dt/2, dt)\n",
    "    n_true = len(x_true)\n",
    "    t_new = dt * np.arange(n_true)\n",
    "    \n",
    "    # Create original (nonuniform) time vector\n",
    "    t_orig = np.linspace(0, tf, n)\n",
    "    \n",
    "    # Create new uniform times\n",
    "    #t_new = np.arange(0, tf + dt/2, dt)  # include final tf\n",
    "    \n",
    "    # Interpolate\n",
    "    x_new = np.interp(t_new, t_orig, X)\n",
    "    y_new = np.interp(t_new, t_orig, Y)\n",
    "    \n",
    "    # Package into DataFrame\n",
    "    df = pd.DataFrame({'t': t_new, 'x': x_new, 'y': y_new})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4108db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def resample_path_nearest(X, Y, tf, dt, x_true, y_true, enforce_monotone=True):\n",
    "    \"\"\"\n",
    "    Resample a dense predicted path (X,Y) onto uniform times by\n",
    "    choosing, at each tracking time, the path point closest to the *actual* (x_true,y_true).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X, Y : array-like\n",
    "        Dense predicted path samples (nearly continuous), length n_pred.\n",
    "    tf : float\n",
    "        Final time (seconds).\n",
    "    dt : float\n",
    "        Tracking interval (seconds).\n",
    "    x_true, y_true : array-like\n",
    "        Actual tracking coordinates at uniform times [0, dt, ..., tf].\n",
    "        Must have length n_true = round(tf/dt)+1.\n",
    "    enforce_monotone : bool\n",
    "        If True, ensure chosen indices along (X,Y) are non-decreasing in time.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame with columns ['t', 'x', 'y'] aligned to tracking times.\n",
    "    \"\"\"\n",
    "    X = np.asarray(X).ravel()\n",
    "    Y = np.asarray(Y).ravel()\n",
    "    x_true = np.asarray(x_true).ravel()\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "\n",
    "    #t_new = np.arange(0, tf + dt/2, dt)\n",
    "    n_true = len(x_true)\n",
    "    # Make times align to actual series length (avoids +1 endpoint)\n",
    "    t_new = dt * np.arange(n_true)\n",
    "    if len(x_true) != len(t_new):\n",
    "        raise ValueError(f\"len(x_true)={len(x_true)} must equal number of uniform steps {len(t_new)}\")\n",
    "\n",
    "    # Pairwise squared distances: shape (n_true, n_pred)\n",
    "    # (Fine for typical n_pred ~ few hundred; avoids extra deps.)\n",
    "    dx = x_true[:, None] - X[None, :]\n",
    "    dy = y_true[:, None] - Y[None, :]\n",
    "    D2 = dx*dx + dy*dy\n",
    "\n",
    "    # Best match index at each uniform time\n",
    "    idx = np.argmin(D2, axis=1)\n",
    "\n",
    "    if enforce_monotone:\n",
    "        # Non-decreasing pass to avoid going \"backwards\" on the predicted path\n",
    "        idx = np.maximum.accumulate(idx)\n",
    "        idx = np.clip(idx, 0, len(X) - 1)\n",
    "\n",
    "    x_new = X[idx]\n",
    "    y_new = Y[idx]\n",
    "    return pd.DataFrame({'t': t_new, 'x': x_new, 'y': y_new})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68033cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_play_paths_2(play_in, play_out, tf, F, arb):\n",
    "\n",
    "    nfl_ids_1 = set(play_in[play_in[\"player_role\"] == 'Defensive Coverage'][\"nfl_id\"])\n",
    "    targetPos = [play_in[\"ball_land_x\"].iloc[0], play_in[\"ball_land_y\"].iloc[0]]\n",
    "    nfl_ids_2 = set(play_out[\"nfl_id\"])\n",
    "    nfl_ids = list(nfl_ids_1 & nfl_ids_2)\n",
    "    #F = 150 \n",
    "    Emax = tf *arb\n",
    "    h = 1.8\n",
    "    m = 80.0\n",
    "    i = 0\n",
    "    #bad_ids = [55910, 53953]\n",
    "    #bad_ids = []\n",
    "    errs = {}\n",
    "    merged_play = pd.DataFrame(columns=['x', 'y'])  # empty default\n",
    "\n",
    "    if len(nfl_ids) == 0:\n",
    "        print(\"NO DEFENSIVE COVERAGE\")\n",
    "        return merged_play, errs\n",
    "        \n",
    "    for nfl_id in nfl_ids:\n",
    "        player_in = play_in.query('nfl_id == ' + str(nfl_id))[[\"x\",\"y\"]]\n",
    "        player_out = play_out.query('nfl_id == ' + str(nfl_id))\n",
    "        goal_list = player_out[[\"x\",\"y\", \"nfl_id\"]]\n",
    "        _, conds = get_initial_conds(play_in, goal_list, nfl_id)\n",
    "        r = 0.2\n",
    "        goal_2 = player_in.iloc[-1]\n",
    "        goal = goal_list.iloc[0]\n",
    "        vx = (goal[0] - goal_2[0]) / 0.1\n",
    "        vy = (goal[1] - goal_2[1]) / 0.1\n",
    "        velo = [vx, vy]\n",
    "        out = matlab_player_path_2(conds['xi'], velo, tf, m, h, Emax, F, targetPos)\n",
    "        #out = matlab_player_path_2(conds['xi'], conds['vi'], conds['xf'], conds['vf'], r)\n",
    "        X = np.array(out['X']).flatten()\n",
    "        Y = np.array(out['Y']).flatten()\n",
    "        df = pd.DataFrame({'x': X, 'y': Y})\n",
    "        df['nfl_id'] = nfl_id\n",
    "\n",
    "        dt = 0.1     # interval (seconds)\n",
    "        x_true = conds['xf'][0]\n",
    "        y_true = conds['xf'][1]\n",
    "\n",
    "        end_pt_err = (x_true - X[-1])**2 + (y_true - Y[-1])**2\n",
    "        errs[f'{nfl_id}'] = end_pt_err\n",
    "        \n",
    "        if i ==0 :\n",
    "            merged_play = df\n",
    "        else:\n",
    "            merged_play = pd.concat([merged_play, df], ignore_index=True)\n",
    "\n",
    "        i +=1\n",
    "\n",
    "    return merged_play, errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386e2f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLP_from_sample(plays_sample, rmse_tracker):\n",
    "    i = 0\n",
    "    for _, play in plays_sample.iterrows():\n",
    "        gid = play[\"game_id\"]\n",
    "        pid = play[\"play_id\"]\n",
    "\n",
    "        play_in = df_in.query('play_id == ' + str(pid) + ' and game_id == ' + str(gid))\n",
    "        play_out = df_out.query('play_id == ' + str(pid) + ' and game_id == ' + str(gid))\n",
    "\n",
    "        if play_out.empty:\n",
    "            continue\n",
    "\n",
    "        last_frame = int(play_out[\"frame_id\"].max())\n",
    "        tf = last_frame / 10.0\n",
    "        F = 200\n",
    "        arb = 150\n",
    "        merged_play_2, errs = get_play_paths_2(play_in, play_out, tf, F, arb)\n",
    "\n",
    "        # --- NEW: record an entry for every nfl_id in this play ---\n",
    "        for nfl_id, se in errs.items():  # se = squared endpoint error\n",
    "            key = f'{gid}_{pid}_{nfl_id}'\n",
    "            rmse_tracker[key] = se  # per-player endpoint RMSE\n",
    "            # or: rmse_tracker[key] = se if you want to keep SE and sqrt later\n",
    "\n",
    "        # keep merged trajectories for visualization\n",
    "        merged_play_2[\"game_id\"] = gid\n",
    "        merged_play_2[\"play_id\"] = pid\n",
    "\n",
    "        if i == 0:\n",
    "            merged_play_3 = merged_play_2\n",
    "        else:\n",
    "            merged_play_3 = pd.concat([merged_play_3, merged_play_2], ignore_index=True)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return merged_play_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56119fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random sample of 100 plays\n",
    "unique_plays = df_out[['game_id','play_id']].drop_duplicates()\n",
    "#random_state=42\n",
    "plays_sample = unique_plays.sample(n=1500)\n",
    "#plays_sample = unique_plays\n",
    "\n",
    "# def \n",
    "# play_in = df_in.query('play_id == 1711 and game_id == 2023090700')\n",
    "# play_out = df_out.query('play_id == 1711 and game_id == 2023090700')\n",
    "# nfl_ids = list(set(play_out[\"nfl_id\"]))\n",
    "\n",
    "# last_frame = int(play_out2[\"frame_id\"].max())\n",
    "# nfl_ids2\n",
    "rmse_tracker4 = {}\n",
    "i = 0\n",
    "\n",
    "for _, play in plays_sample.iterrows():\n",
    "    gid = play[\"game_id\"]\n",
    "    pid = play[\"play_id\"]\n",
    "\n",
    "    play_in = df_in.query('play_id == ' + str(pid) + ' and game_id == ' + str(gid))\n",
    "    play_out = df_out.query('play_id == ' + str(pid) + ' and game_id == ' + str(gid))\n",
    "\n",
    "    if play_out.empty:\n",
    "        continue\n",
    "\n",
    "    last_frame = int(play_out[\"frame_id\"].max())\n",
    "    tf = last_frame / 10.0\n",
    "\n",
    "    F = 200\n",
    "    arb = 150\n",
    "    merged_play_2, errs = get_play_paths_2(play_in, play_out, tf, F, arb)\n",
    "\n",
    "    # --- NEW: record an entry for every nfl_id in this play ---\n",
    "    for nfl_id, se in errs.items():  # se = squared endpoint error\n",
    "        key = f'{gid}_{pid}_{nfl_id}'\n",
    "        rmse_tracker4[key] = se  # per-player endpoint RMSE\n",
    "        # or: rmse_tracker[key] = se if you want to keep SE and sqrt later\n",
    "\n",
    "    # keep merged trajectories for visualization\n",
    "    merged_play_2[\"game_id\"] = gid\n",
    "    merged_play_2[\"play_id\"] = pid\n",
    "\n",
    "    if i == 0:\n",
    "        merged_play_3 = merged_play_2\n",
    "    else:\n",
    "        merged_play_3 = pd.concat([merged_play_3, merged_play_2], ignore_index=True)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "#np.sqrt(0.5*merged_play_3[\"se\"].mean())  \n",
    "all_rmse = np.array(list(rmse_tracker4.values()))\n",
    "overall_rmse = np.sqrt(0.5*all_rmse.mean())\n",
    "print(\"Overall mean endpoint RMSE:\", overall_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6353b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_tracker7_new = {}\n",
    "i = 0\n",
    "\n",
    "#for _, row in sample_50.iterrows():\n",
    "for _, row in remaining_df.iterrows():\n",
    "    gid = int(row[\"game_id\"])\n",
    "    pid = int(row[\"play_id\"])\n",
    "\n",
    "    play_in = df_in.query('play_id == ' + str(pid) + ' and game_id == ' + str(gid))\n",
    "    play_out = df_out.query('play_id == ' + str(pid) + ' and game_id == ' + str(gid))\n",
    "\n",
    "    if play_out.empty:\n",
    "        continue\n",
    "\n",
    "    last_frame = int(play_out[\"frame_id\"].max())\n",
    "    tf = last_frame / 10.0\n",
    "\n",
    "    F = 200\n",
    "    arb = 150\n",
    "    merged_play_2, errs = get_play_paths_2(play_in, play_out, tf, F, arb)\n",
    "\n",
    "    # --- NEW: record an entry for every nfl_id in this play ---\n",
    "    for nfl_id, se in errs.items():  # se = squared endpoint error\n",
    "        key = f'{gid}_{pid}_{nfl_id}'\n",
    "        rmse_tracker7_new[key] = se  # per-player endpoint RMSE\n",
    "        # or: rmse_tracker[key] = se if you want to keep SE and sqrt later\n",
    "\n",
    "    # keep merged trajectories for visualization\n",
    "    merged_play_2[\"game_id\"] = gid\n",
    "    merged_play_2[\"play_id\"] = pid\n",
    "\n",
    "    if i == 0:\n",
    "        merged_play_13 = merged_play_2\n",
    "    else:\n",
    "        merged_play_13 = pd.concat([merged_play_3, merged_play_2], ignore_index=True)\n",
    "\n",
    "    i += 1\n",
    "    print(\"Play i: \", i)\n",
    "\n",
    "#np.sqrt(0.5*merged_play_3[\"se\"].mean())  \n",
    "all_rmse_new = np.array(list(rmse_tracker7_new.values()))\n",
    "overall_rmse_new = np.sqrt(0.5*all_rmse_new.mean())\n",
    "print(\"Overall mean endpoint RMSE:\", overall_rmse_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c58774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f911b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, play in df_trials.iterrows():\n",
    "    game_id = play[\"game_id\"]\n",
    "    play_id = play[\"play_id\"]\n",
    "    merged_top = trial_df_plays.query('play_id ==' + str(play_id) + ' and game_id == ' + str(game_id))\n",
    "    plot_play(game_id, play_id, True, merged_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02a8914",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 49410 2023111904 299\n",
    "\n",
    "# 53589 2023101509 2672\n",
    "\n",
    "# 56074 2023102204 723\n",
    "test_list = [(49410, 2023111904, 299), (53589, 2023101509, 2672), (56074, 2023102204, 723)]\n",
    "for nfl_id, gid, pid in test_list:\n",
    "    play_in = df_in.query('play_id == ' + str(pid) + ' and game_id == ' + str(gid))\n",
    "    play_out = df_out.query('play_id == ' + str(pid) + ' and game_id == ' + str(gid))\n",
    "    targetPos = [play_in[\"ball_land_x\"].iloc[0], play_in[\"ball_land_y\"].iloc[0]]\n",
    "    player_in = play_in.query('nfl_id == ' + str(nfl_id))[[\"x\",\"y\"]]\n",
    "    player_out = play_out.query('nfl_id == ' + str(nfl_id))\n",
    "    goal_list = player_out[[\"x\",\"y\", \"nfl_id\"]]\n",
    "    _, conds = get_initial_conds(play_in, goal_list, nfl_id)\n",
    "    r = 0.2\n",
    "    last_frame = int(play_out[\"frame_id\"].max())\n",
    "    tf = last_frame / 10.0\n",
    "    goal_2 = player_in.iloc[-1]\n",
    "    goal = goal_list.iloc[0]\n",
    "    vx = (goal[0] - goal_2[0]) / 0.1\n",
    "    vy = (goal[1] - goal_2[1]) / 0.1\n",
    "    velo = [vx, vy]\n",
    "    print(\"COMBO 1 conds: \", conds, \" velo: \", velo, \" target: \", targetPos, \" tf: \", tf)\n",
    "    print(f\"goal_list_{nfl_id}_{pid}.csv\")\n",
    "    goal_list.to_csv(f\"goal_list_{nfl_id}_{pid}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ecac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_rmse = {}\n",
    "for d in [rmse_tracker, rmse_tracker2, rmse_tracker3, rmse_tracker4]:\n",
    "    merged_rmse.update(d)\n",
    "\n",
    "# get all RMSE values\n",
    "values = list(merged_rmse.values())\n",
    "\n",
    "# plot histogram\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(values, bins=30, edgecolor='black')\n",
    "plt.xlabel(\"RMSE\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of RMSE across all trackers\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15167ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_play_paths(play_in, play_out, tf):\n",
    "    nfl_ids = list(set(play_out[\"nfl_id\"]))\n",
    "    i = 0\n",
    "    #bad_ids = [55910, 53953]\n",
    "    #bad_ids = []\n",
    "    for nfl_id in nfl_ids:\n",
    "        player_in = play_in.query('nfl_id == ' + str(nfl_id))[[\"x\",\"y\"]]\n",
    "        player_out = play_out.query('nfl_id == ' + str(nfl_id))\n",
    "        goal_list = player_out[[\"x\",\"y\", \"nfl_id\"]]\n",
    "        _, conds = get_initial_conds(play_in, goal_list, nfl_id)\n",
    "        r = 0.2\n",
    "        out = matlab_player_path(conds['xi'], conds['vi'], conds['xf'], conds['vf'], r)\n",
    "        X = np.array(out['X']).flatten()\n",
    "        Y = np.array(out['Y']).flatten()\n",
    "        dt = 0.1     # interval (seconds)\n",
    "        x_true = goal_list[\"x\"].to_numpy()\n",
    "        y_true = goal_list[\"y\"].to_numpy()\n",
    "        \n",
    "        df_path = resample_path_nearest(X, Y, tf, dt, x_true, y_true)\n",
    "        #df_path = resample_path(X, Y, tf, dt, x_true, y_true)\n",
    "        pred = df_path[[\"x\",\"y\"]].to_numpy()\n",
    "        true = goal_list[[\"x\",\"y\"]].to_numpy()\n",
    "\n",
    "        df_path[\"se\"] = ((pred - true)**2).sum(axis=1)\n",
    "        \n",
    "        df_path[\"nfl_id\"] = nfl_id\n",
    "        #df_path[\"mse\"] = mean_squared_error(df_path['x'], goal_list['x']) + mean_squared_error(df_path['y'], goal_list['y'])\n",
    "        #df_path[\"se\"] = ((df_path['x'] - goal_list['x'])**2) + ((df_path['y'] - goal_list['y'])**2) \n",
    "        rmse_e = rmse(df_path, goal_list, tf*10)\n",
    "        overall_rmse = np.sqrt(df_path[\"se\"].mean())\n",
    "        print(nfl_id, \" Error: \", rmse_e, \" new_error: \", overall_rmse)\n",
    "        goal_2 = player_in.iloc[-1]\n",
    "        goal = goal_list.iloc[0]\n",
    "        vx = (goal[0] - goal_2[0]) / 0.1\n",
    "        vy = (goal[1] - goal_2[1]) / 0.1\n",
    "        velo = [vx, vy]\n",
    "        out2 = matlab_player_path(conds['xi'], velo, conds['xf'], conds['vf'], r)\n",
    "        X2 = np.array(out2['X']).flatten()\n",
    "        Y2 = np.array(out2['Y']).flatten()\n",
    "        df_path2 = resample_path_nearest(X2, Y2, tf, dt, x_true, y_true)\n",
    "        #df_path2 = resample_path(X2, Y2, tf, dt, x_true, y_true)\n",
    "        pred2 = df_path2[[\"x\",\"y\"]].to_numpy()\n",
    "\n",
    "        df_path2[\"se\"] = ((pred2 - true)**2).sum(axis=1)\n",
    "        #df_path2[\"mse\"] = mean_squared_error(df_path2['x'], goal_list['x']) + mean_squared_error(df_path2['y'], goal_list['y'])\n",
    "        #df_path2[\"se\"] = ((df_path2['x'] - goal_list['x'])**2) + ((df_path2['y'] - goal_list['y'])**2) \n",
    "        \n",
    "        df_path2[\"nfl_id\"] = nfl_id\n",
    "        rmse_e2 = rmse(df_path2, goal_list, tf*10)\n",
    "        overall_rmse2 = np.sqrt(df_path2[\"se\"].mean())\n",
    "        print(nfl_id, \" Error2: \", rmse_e2, \" new_error2: \", overall_rmse2)\n",
    "        #if overall_rmse > overall_rmse2 and nfl_id not in bad_ids:\n",
    "        if overall_rmse > overall_rmse2:\n",
    "            df_path = df_path2\n",
    "        # if nfl_id in bad_ids:\n",
    "        #     df_path = df_path2\n",
    "        \n",
    "        if i ==0 :\n",
    "            merged_play = df_path\n",
    "        else:\n",
    "            merged_play = pd.concat([merged_play, df_path], ignore_index=True)\n",
    "\n",
    "        i +=1\n",
    "\n",
    "    return merged_play\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bdeb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random sample of 100 plays\n",
    "unique_plays = df_out[['game_id','play_id']].drop_duplicates()\n",
    "# random_state=42\n",
    "#plays_sample = unique_plays.sample(n=1000)\n",
    "plays_sample = unique_plays\n",
    "\n",
    "# def \n",
    "# play_in = df_in.query('play_id == 1711 and game_id == 2023090700')\n",
    "# play_out = df_out.query('play_id == 1711 and game_id == 2023090700')\n",
    "# nfl_ids = list(set(play_out[\"nfl_id\"]))\n",
    "\n",
    "# last_frame = int(play_out2[\"frame_id\"].max())\n",
    "# nfl_ids2\n",
    "plays_sample\n",
    "rmse_tracker = {}\n",
    "i = 0\n",
    "for _, play in plays_sample.iterrows():\n",
    "    gid = play[\"game_id\"]\n",
    "    pid = play[\"play_id\"]\n",
    "    #print(play[\"game_id\"])\n",
    "    play_in = df_in.query('play_id ==' + str(pid) +' and game_id == ' + str(gid))\n",
    "    play_out = df_out.query('play_id ==' + str(pid) +' and game_id == ' + str(gid))\n",
    "    nfl_ids = list(set(play_out[\"nfl_id\"]))\n",
    "\n",
    "    last_frame = int(play_out2[\"frame_id\"].max())\n",
    "    tf = last_frame/10\n",
    "    merged_play_2 = get_play_paths(play_in, play_out, tf)\n",
    "    overall_rmse = np.sqrt(0.5*merged_play_2[\"se\"].mean())\n",
    "    rmse_tracker[f'{gid}_{pid}'] = overall_rmse\n",
    "    merged_play_2[\"game_id\"] = gid\n",
    "    merged_play_2[\"play_id\"] = pid\n",
    "    if i == 0:\n",
    "        merged_play_3 = merged_play_2\n",
    "    else:\n",
    "        merged_play_3 = pd.concat([merged_play_3, merged_play_2], ignore_index=True)\n",
    "\n",
    "    i+=1\n",
    "\n",
    "overall_rmse = np.sqrt(0.5*merged_play_3[\"se\"].mean())   \n",
    "print(overall_rmse) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b38eafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# joblib.dump({\n",
    "#     'preds_matlab': merged_play_3\n",
    "# }, 'preds_matlab.joblib')\n",
    "\n",
    "# print(\"Saved preds to preds_matlab.joblib\")\n",
    "#merged_play_3\n",
    "merged_play_3 = joblib.load('preds_matlab.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eeaa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_play(game_id, play_id, oof, merged_play_2):\n",
    "    data_in  = df_in.query('game_id == ' + str(game_id) + ' and play_id == ' + str(play_id)).copy()\n",
    "    data_out = df_out.query('game_id == ' + str(game_id) + ' and play_id == ' + str(play_id)).copy()\n",
    "    preds_temp = preds_lstm\n",
    "    # if oof:\n",
    "    #     preds_temp = preds_oof_lstm\n",
    "    # else:\n",
    "    #     preds_temp = preds_lstm\n",
    "\n",
    "    preds_out = preds_temp.query('game_id == ' + str(game_id) + ' and play_id == ' + str(play_id)).copy()\n",
    "    # --- 2) Define a minimal, consistent schema and a standardizer\n",
    "    MIN_COLS = ['game_id','play_id','frame_id','x','y','nfl_id','player_role']\n",
    "\n",
    "    def standardize(df, source_label):\n",
    "        df = df.copy()\n",
    "        # Ensure all expected columns exist (create safe defaults if missing)\n",
    "        for c in MIN_COLS:\n",
    "            if c not in df.columns:\n",
    "                if c == 'player_role':\n",
    "                    if source_label == 'out':\n",
    "                        df[c] = 'def_air'\n",
    "                    else: \n",
    "                        df[c] = 'pred_def_air'\n",
    "                elif c in ('x','y'):\n",
    "                    df[c] = np.nan\n",
    "                else:\n",
    "                    df[c] = pd.NA\n",
    "\n",
    "        # Coerce numeric columns\n",
    "        for c in ['x','y','frame_id','nfl_id']:\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "        # Normalize player_role dtype\n",
    "        if source_label == 'out':\n",
    "            df['player_role'] = df['player_role'].astype(str).fillna('def_air')\n",
    "            # If data_in provided, copy role for targeted receiver\n",
    "            if data_in is not None and 'player_role' in data_in.columns:\n",
    "                # Get the nfl_ids of targeted receivers\n",
    "                targeted_ids = data_in.loc[\n",
    "                    data_in['player_role'] == 'Targeted Receiver', 'nfl_id'\n",
    "                ].unique()\n",
    "\n",
    "                # If any targeted receivers exist, copy the label\n",
    "                df.loc[df['nfl_id'].isin(targeted_ids), 'player_role'] = 'targeted_air'\n",
    "            # if data_in['nfl_id']:\n",
    "            #     data_in[data_in['nfl_id'] == 55910]['player_role'].iloc[0]\n",
    "        \n",
    "\n",
    "            \n",
    "        # Tag source\n",
    "        df['source'] = source_label\n",
    "        # Keep only the minimal columns + source to avoid dtype merge issues\n",
    "        return df[['game_id','play_id','frame_id','x','y','nfl_id','player_role','source']]\n",
    "\n",
    "    data_in_std  = standardize(data_in,  'in')\n",
    "    data_out_std = standardize(data_out, 'out')\n",
    "    pred_out_std = standardize(preds_out, 'pred')\n",
    "\n",
    "    # --- 3) Concatenate safely\n",
    "    merged_play = pd.concat([data_in_std, data_out_std], ignore_index=True)\n",
    "    g = sns.pairplot(\n",
    "        merged_play,\n",
    "        x_vars=[\"x\"],\n",
    "        y_vars=[\"y\"],\n",
    "        height=3.5,\n",
    "        hue=\"player_role\" #hue define the color-code variable\n",
    "    )\n",
    "    plt.title('game_id = ' + str(game_id) + ' and play_id = ' + str(play_id))\n",
    "    #plt.show()\n",
    "\n",
    "    # ---- Add the ball as a single 'x' ----\n",
    "    ball_x = data_in[\"ball_land_x\"].iloc[0]\n",
    "    ball_y = data_in[\"ball_land_y\"].iloc[0]\n",
    "\n",
    "    # `g.axes[0,0]` gives you the Axes object for this 1Ã—1 pairplot\n",
    "    ax = g.axes[0, 0]\n",
    "\n",
    "    # last_frame = int(data[\"frame_id\"].max())\n",
    "\n",
    "    # last_pts = data[data[\"frame_id\"] == last_frame]\n",
    "    first_frame = int(data_in[\"frame_id\"].max())\n",
    "    first_pts = data_in[data_in[\"frame_id\"] == first_frame]\n",
    "\n",
    "    # ax.scatter(\n",
    "    #     last_pts[\"x\"], last_pts[\"y\"],\n",
    "    #     marker=\"x\", s=100, linewidths=2,\n",
    "    #     c=\"blue\", label=f\"df_in last frame ({last_frame})\"\n",
    "    # )\n",
    "    ax.scatter(ball_x, ball_y, marker=\"x\", s=120, color=\"black\", label=\"Ball\")\n",
    "    # ax.scatter(\n",
    "    #     pred_out_std[\"x\"],\n",
    "    #     pred_out_std[\"y\"],\n",
    "    #     marker=\"*\",           # or \"o\" / \"*\" / \"^\" etc.\n",
    "    #     s=5,\n",
    "    #     c=\"blue\",\n",
    "    #     alpha=0.8,\n",
    "    #     label=\"Predicted (df_out)\"\n",
    "    # )\n",
    "    ax.scatter(\n",
    "        merged_play_2[\"x\"],\n",
    "        merged_play_2[\"y\"],\n",
    "        marker=\"*\",           # or \"o\" / \"*\" / \"^\" etc.\n",
    "        s=5,\n",
    "        c=\"red\",\n",
    "        alpha=0.8,\n",
    "        label=\"Predicted (df_out)\"\n",
    "    )\n",
    "    \n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
